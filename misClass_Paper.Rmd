---
title: Regression modelling based on imperfect labels arising from unsupervised learning
  by Gaussian mixture models
author: "Rasmus Brøndum, Thomas Yssing Michalsen & Martin Bøgsted"
date: "20 11 2019"
output:
  html_document: default
  pdf_document: default
---
```{r setup, include=FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::knit_engines$set(python = reticulate::eng_python)

# Install simex package with cox
#devtools::install_github("https://github.com/HaemAalborg/simex-1", force = T)

# Packages from CRAN
library(knitr, quietly = T)
library(dplyr, quietly = T)
library(mclust, quietly = T)
library(mixtools, quietly = T)
library(simex, quietly = T)
library(survival, quietly = T)
library(arrangements, quietly = T)
library(foreach)
library(doParallel)
library(doRNG)
library(Hmisc)
library(survminer)
library(magrittr)
library(tidyverse)
library(kableExtra)
library(GFORCE)

# Packages from Bioconductor
library(Biobase)
library(GEOquery)
library(affy)
library(hgu133plus2cdf)
```

# Auxillary functions
```{r auxillary functions}
# The k-means function does not come with a predict method in its name space
# you have to build your own. A suggestion called predict.kmeans is given 
# at the home page
#
# https://stats.stackexchange.com/questions/12623/predicting-cluster-of-a-new-object-with-kmeans-in-r
#
predict.kmeans <- function(object,
                           newdata,
                           method = c("centers", "classes")) {
  method <- match.arg(method)
  
  centers <- object$centers
  ss_by_center <- apply(centers, 1, function(x) {
    colSums((t(newdata) - x) ^ 2)
  })
  best_clusters <- apply(ss_by_center, 1, which.min)
  
  if (method == "centers") {
    centers[best_clusters, ]
  } else {
    best_clusters
  }
}

# Reordering a confusion matrix to optimize accuracy
opt <- function(x){
  acc <- function(x)
  {
    sum(diag(x))/sum(x)
  }
  pe <- permutations(7,7)
  curr.x <- acc(x)
  curr.i <- 1:7
  for(i in 1:nrow(pe)){
    if(acc(x[pe[i,],])>curr.x){ 
      curr.i = pe[i,]
      curr.x <- acc(x[pe[i,],])
    }
  }
  curr.i
}

# Function to estimate the misclassification matrix from a Gaussian mixture model
# fitted by Mclust by Monte Carlo integration or kmeans.
Pi <- function(fit){
  if(class(fit) == "Mclust"){
    cl  <- unique(fit$classification)
    n.cl <- length(cl)
    hatPi <- matrix(0, n.cl, n.cl)
    for(j in cl){
      x <- rmvnorm(100000, fit$parameters$mean[,j], fit$parameters$variance$sigma[ , , j])
      p.x <- predict(fit,x)$classification
      for(i in cl){
        hatPi[i,j] <- mean(p.x == i)
      }
    }
    reord <- order(fit$parameters$mean[1,])
    hatPi <- hatPi[reord, reord]
  }
  if(class(fit) == "kmeans"){
    cl <- unique(fit$cluster)
    n.cl <- length(cl)
    hatPi <- matrix(0, n.cl, n.cl)
    sds <- sqrt(fit$withinss / fit$size)
    for(j in cl){
      x <- rmvnorm(100000, fit$centers[j,], diag(sds[j],2))
      p.x <- predict.kmeans(fit, newdata = x, method = "classes")
      for(i in cl){
        hatPi[i,j] <- mean(p.x == i)
      }
    }
    reord <- order(fit$centers[,1])
    hatPi <- hatPi[reord, reord]
  }
  hatPi
}


# Function to "indicate" if observed endpoints of intevals cover the right value,
# i.e. a component in coverage calculations.
ind <- function(x){
  res <- c(0,0,0)
  if(x[1] >= x[2] & x[1] <= x[3]){
    res[1] <- 1
  } else{
    res[1] <- 0
  }
  if(x[1] >= x[4] & x[1] <= x[5]){
    res[2] <- 1
  } else{
    res[2] <- 0
  }
  if(is.na(x[6])){
    res[3] <- NA
  } else{
    if (x[1] >= x[6] & x[1] <= x[7]) 
      res[3] <- 1
    else 
      res[3] <- 0
  }
  return(res)
}

# Function for simulating survival data.
my.sim <- function(n, mp = 0.3){
  X1 <- rbinom(n, size = 1, p = 0.5) 
  X2 <- 2*rbinom(n, size = 1, p = mp) + X1
  X2 <- case_when(X2 == 0 ~ 0,
            X2 == 1 ~ 1,
            X2 == 2 ~ 1,
            X2 == 3 ~ 0)
  eventtime <- rexp(n, (X1+1))
  censtime <- rexp(n, 0.5)
  time <- pmin(eventtime, censtime)
  status <- eventtime < censtime
  event <- status
  X1  <- factor(X1)
  X2  <- factor(X2)
  data.frame(eventtime = eventtime, censtime = censtime, time = time, event = event,
             X1 = X1, X2 = X2,status = status)
}

## Function for binomial simulations
simBinom <- function(pi0 = 5/10,      # Probability of 1. component
                     mu0 = c(-1, 0),  # Mean parameter of feature 1
                     mu1 = c(1, 0),   # Mean parameter of feature 2
                     alpha = -1,      # 
                     beta = 2,        #
                     n = 1000,        # Number of samples
                     method = "GMM"){ # Method for clustering GMM or Kmeans
  
  # Probability of 2. component
  pi1 = 1 - pi0 
  
  # Results container
  results <- rep(NA,12)
  names(results) <- c("true.a", "true.a.std", "true.b", "true.b.std",
                      "naive.a", "naive.a.std", "naive.b", "naive.b.std",
                      "mcsimex.a", "mcsimex.a.std", "mcsimex.b", "mcsimex.b.std")
  
  # Training data, classes and features
  class <- factor(sort(rbinom(n, size = 1, prob = pi1)+1))
  feature <- rbind(rmvnorm(sum(class == 1), mu0),
                   rmvnorm(sum(class == 2), mu1))
  
  # Outcome linearly regressed to class
  outcome   <- rbinom(n, size = 1, 
                      p = exp(alpha + 
                                (beta)*(class==2))/(1+exp(alpha + (beta)*(class==2))))
  
  # Dataframe containing training data
  train <- data.frame(id = 1:n, class = class, feature = feature, 
                      outcome = outcome)
  if(method == "GMM"){
    # Fit Gaussian mixture model with two components
    fit <- Mclust(train[,c("feature.1", "feature.2")], G = 2)
    
    # Classfication of training data
    pred <- predict(fit)
    reord <- order(fit$parameters$mean[1,])
    train$predicted <- factor(pred$classification)
    train$predicted <- relevel(factor(train$predicted, label = reord), "1")
  }
  
  if(method == "Kmeans"){
    # Fit K.means with two clusters
    fit <- kmeans(train[,c("feature.1", "feature.2")], 2)
    
    
    # Classification of training data
    pred <- predict.kmeans(fit, newdata = train[,c("feature.1", "feature.2")],
                           method = "classes")
    reord <- order(fit$centers[,1])
    train$predicted <- factor(pred)
    train$predicted <- relevel(factor(train$predicted, label = reord), "1")
  }
  
  ## Check if predicted classes has two levels, if not return NA results
  if(length(unique(train$predicted)) == 2){
    res <- table(train$predicted, train$class)
    
    err <- 1-sum(diag(res))/sum(res)
    
    hatPi <- Pi(fit)
    
    dimnames(hatPi) <- list(levels(train$predicted), levels(train$class))
    
    true <- glm(outcome ~ class, data = train, family = binomial, x = T, y = T)
    results[c("true.a","true.b")] <- coefficients(true)
    results[c("true.a.std","true.b.std")] <- sqrt(diag(vcov(true)))
    
    naive <- glm(outcome ~ predicted, data = train, family = binomial, x = T, y =T)
    results[c("naive.a","naive.b")] <- coefficients(naive)
    results[c("naive.a.std","naive.b.std")] <- sqrt(diag(vcov(naive)))
    
    # Simex
    # This occationally fails if class is very unbalanced in which case
    # Simex seems to reassign everything to one class
    fit.class <- try(mcsimex(naive, mc.matrix = hatPi, SIMEXvariable = "predicted"),
                     silent = TRUE)
    if(class(fit.class) == "mcsimex"){
      results[c("mcsimex.a","mcsimex.b")] <- coefficients(fit.class)
      results[c("mcsimex.a.std","mcsimex.b.std")] <- sqrt(diag(fit.class$variance.asymptotic))
    }
  }
  return(results)
}

## Function to summarize results from binomial simulations
summarizeResults <- function(results, alpha, beta, digits){
  ## Bias
  bias.a <- colMeans(results[,c("true.a", "naive.a", "mcsimex.a")] - alpha, na.rm = TRUE)
  bias.b <- colMeans(results[,c("true.b", "naive.b", "mcsimex.b")] - beta, na.rm = TRUE)
  ## MSE
  mse.a <- colMeans((results[,c("true.a", "naive.a", "mcsimex.a")] - alpha)^2, na.rm = TRUE)
  mse.b <- colMeans((results[,c("true.b", "naive.b", "mcsimex.b")] - beta)^2, na.rm = TRUE)
  
  meanNA <- function(x) mean(x, na.rm = T)
  ## Coverage alpha
  confint.a <- cbind(alpha,
        results[,c("true.a")]-1.96*results[,c("true.a.std")],
        results[,c("true.a")]+1.96*results[,c("true.a.std")],
        results[,c("naive.a")]-1.96*results[,c("naive.a.std")],
        results[,c("naive.a")]+1.96*results[,c("naive.a.std")],
        results[,c("mcsimex.a")]-1.96*results[,c("mcsimex.a.std")],
        results[,c("mcsimex.a")]+1.96*results[,c("mcsimex.a.std")])
  coverage.a <- apply(t(apply(confint.a, 1, ind)), 2, meanNA)
  
  ## Coverage beta
  confint.b <- cbind(beta,
        results[,c("true.b")]-1.96*results[,c("true.b.std")],
        results[,c("true.b")]+1.96*results[,c("true.b.std")],
        results[,c("naive.b")]-1.96*results[,c("naive.b.std")],
        results[,c("naive.b")]+1.96*results[,c("naive.b.std")],
        results[,c("mcsimex.b")]-1.96*results[,c("mcsimex.b.std")],
        results[,c("mcsimex.b")]+1.96*results[,c("mcsimex.b.std")])
  coverage.b <- apply(t(apply(confint.b, 1, ind)), 2, meanNA)
  
  return(round(rbind(bias.a,bias.b,mse.a,mse.b,coverage.a,coverage.b), digits))
}


summarizeResultsCox <- function(results, HR = 2, digits = 2){
  ## Bias
  bias <- colMeans(exp(results[,c("True", "Naive", "Simex")]) - HR, na.rm = TRUE)
  ## MSE
  mse <- colMeans((exp(results[,c("True", "Naive", "Simex")]) - HR)^2, na.rm = TRUE)
  ## Coverage
  meanNA <- function(x) mean(x, na.rm = T)
  confint <- cbind(log(HR),
                   results[,c("True")]-1.96*results[,c("True.sd")],
                   results[,c("True")]+1.96*results[,c("True.sd")],
                   results[,c("Naive")]-1.96*results[,c("Naive.sd")],
                   results[,c("Naive")]+1.96*results[,c("Naive.sd")],
                   results[,c("Simex")]-1.96*results[,c("Simex.sd")],
                   results[,c("Simex")]+1.96*results[,c("Simex.sd")])
  coverage <- apply(t(apply(confint, 1, ind)), 2, meanNA)
  return(round(rbind(bias, mse, coverage), digits))
}

bootConf <- function(n.boot, data, orig.class, method = "GMM"){
  n <- nrow(data)
  if(n.boot != FALSE){
    boot = sort(sample(1:n, size = n.boot, replace = FALSE))
  } else{
    boot = sort(sample(1:n, size = n, replace = TRUE))
  }
  bag = data[boot,]
  outofbag = data[-boot,]
  ct.outofbag <- orig.class[-boot]
  
  if(method == "GMM"){
    fit.bag = Mclust(bag, G = 7)
    pred.outofbag <- predict(fit.bag, outofbag)
    ctpred.outofbag <- pred.outofbag$classification
  }
  if(method =="Kmeans"){
    fit.bag = kmeans(bag, 7)
    pred.outofbag <- predict.kmeans(fit.bag, outofbag)
    ctpred.outofbag = dimnames(pred.outofbag)[[1]]
    names(ctpred.outofbag) <- rownames(pred.outofbag$z)
  }
  
  o.boot = table(ctpred.outofbag, ct.outofbag)
  nr <- nrow(o.boot)
  if(nr < 7) o.boot <- rbind(o.boot,matrix(0,ncol=ncol(o.boot),nrow=7-nr))
  nc <- ncol(o.boot)
  if(nc < 7) o.boot <- cbind(o.boot,matrix(0,nrow=nrow(o.boot),ncol=7-nc))
  o.boot = o.boot[opt(o.boot),]
  return(o.boot)
}

# Uncentered correlation coefficient
ucor <- function(x,y, offSet = 0){
  phiX <- sqrt(mean((x - offSet)^2))
  phiY <- sqrt(mean((y - offSet)^2))
  mean(((x - offSet) / phiX) * ((y - offSet) / phiY))
}

simexBoot <- function(n.boot,
                      orig.class,
                      edata,
                      pdata,
                      SIMEXvar){
  
  ### Bootstrap MC matrix
  n <- nrow(edata)
  if(n.boot != FALSE){
    boot = sort(sample(1:n, size = n.boot, replace = FALSE))
  } else{
    boot = sort(sample(1:n, size = n, replace = TRUE))
  }
  bag = edata[boot,]
  outofbag = edata[-boot,]
  ct.outofbag <- orig.class[-boot]
  
  fit.bag = Mclust(bag, G = 7)
  pred.outofbag <- predict(fit.bag, outofbag)
  ctpred.outofbag <- pred.outofbag$classification
  
  o.boot = table(ctpred.outofbag, ct.outofbag)
  nr <- nrow(o.boot)
  if(nr < 7) o.boot <- rbind(o.boot,matrix(0,ncol=ncol(o.boot),nrow=7-nr))
  nc <- ncol(o.boot)
  if(nc < 7) o.boot <- cbind(o.boot,matrix(0,nrow=nrow(o.boot),ncol=7-nc))
  o.boot = o.boot[opt(o.boot),]
  
  ## Build misclassification  matrix for two classes
  high <- which(colnames(o.boot) %in% c("PR", "MF", "MS"))
  hatPi <- matrix(data = c(sum(o.boot[-high,-high]),
                                 sum(o.boot[-high,high]),
                                 sum(o.boot[high,-high]),
                                 sum(o.boot[high,high])),
                        ncol = 2, nrow = 2, byrow = T)
  
  hatPi <- sweep(hatPi, MARGIN = 2, FUN="/", STATS=colSums(hatPi))
  row.names(hatPi) <- colnames(hatPi) <- c("Low Risk", "High Risk")
  
  ## Fit naive model on bootstrap sample
  cox.gmm.naive  <- coxph(OS~gmm.risk, model = TRUE, data = pdata[boot,])
  
  ## Fit simex
  cox.gmm.simex <- mcsimex(cox.gmm.naive,
                           SIMEXvariable = SIMEXvar,
                           mc.matrix = smallXhatPi,
                           asymptotic = FALSE)
  return(cox.gmm.simex$coefficients)
}
```

# Simulation with logistic regression
We generate $n = 1000$ independent training data pairs from a Gaussian mixture model, where the prior probabilities of classes 0 and 1 are $\pi_0 = 5/10$ and $\pi_1 = 5/10$ respectively; class 0 and 1 observations have bivariate normal distributions with means $\mu_0=(-1, 0)$ and $\mu_1=(1, 0)$ respectively, and common identity covariance matrix. The outcome is modelled by logistic regression with linear predictor $\alpha + \beta *\mbox{class}$, where $\alpha=-1$ and $\beta=2$.  

```{r GMM}
# Graphical parameters for this chunck
par(pty = "s")

# Set seed for reproducibility purposes
set.seed(2000)

# Model parameters
pi0 <- 5/10     # Probabiliyt of 1. component
pi1 <- 1 - pi0  # Probability of 2. component
mu0 <- c(-1, 0) # Mean parameter of feature 1
mu1 <- c(1, 0)  # Mean parameter of feature 2
alpha <- -1     # 
beta <- 2       #

# Number of samples in each simluation
n <- 1000

# Simulation and plot of a typical case
class <- factor(sort(rbinom( n = n, size = 1, prob = pi1)+1))
feature <- rbind(rmvnorm(sum(class == 1), mu0),
                 rmvnorm(sum(class == 2), mu1))

# Set plot colours
ccol = ifelse(class == 1, "red", "black")

plot(feature[, 1], feature[, 2], col = ccol, pch = 16, 
     xlim = c(-3, 3), ylim = c(-3, 3), xlab = "Feature 1", ylab = "Feature 2")

```


Obviously, a number of observations will be mis-classfied, so by simulation we investigate the performace of the MC-SIMEX procedure. First we plot a figure showing the principle behind the SIMEX procedure:
```{r}
# Simulate data with 20% misclassification
set.seed(1008)
X1 <- rbinom(100, size = 1, p = 0.5) +1
X2 <- 2*rbinom(100, size = 1, p = 0.2) + X1
X2 <- case_when(X2 == 1 ~ 1,
                X2 == 2 ~ 2,
                X2 == 3 ~ 2,
                X2 == 4 ~ 1)

# Simulate outcome based on true classes
outcome   <- rbinom(100, size = 1, 
                    p = exp(-1 + 2*(X1==2))/(1+exp(-1 + 2*(X1==2))))

# Create dataframe and misclassification matrix
train <- data.frame(id = 1:100,
                    "X1" = factor(X1),
                    "X2" = factor(X2),
                    outcome)
hatPi <- matrix(c(0.8,0.2,0.2,0.8), ncol =2, byrow = T)
dimnames(hatPi) <- list(levels(train$X1), levels(train$X1))

# Fit Naive model
naive <- glm(outcome ~ X2, data = train, family = binomial, x = T, y =T)

# Simex correct for misclassification
fit.class <- mcsimex(naive, mc.matrix = hatPi, SIMEXvariable = "X2")

# Extract data for plot
plotEst    <- data.frame(fit.class$SIMEX.estimates)
plotPoints <- data.frame("lambda"    = rep(fit.class$lambda[-1], each = 100),
                         "Bootstrap" = unlist(fit.class$theta$X22))
lambda     <- seq(-1,max(plotEst$lambda), by = 0.01)
plotLine   <- data.frame(lambda,
                         (predict(fit.class$extrapolation, newdata = data.frame(lambda))))

# Plot SIMEX figure                     
simex.plot <- ggplot(plotPoints, aes(x = lambda + 1, y = Bootstrap)) +
  geom_point(col = "grey") +
  geom_point(data = subset(plotEst, lambda>=0), aes(x = lambda + 1, y = X22), size = 3) +
  geom_point(data = subset(plotEst, lambda<0), aes(x = lambda + 1, y = X22), size = 3, pch = 1) +
  geom_line(data = subset(plotLine, lambda > 0), aes(x = lambda + 1, y = X22)) +
  geom_line(data = subset(plotLine, lambda < 0), aes(x = lambda + 1, y = X22), linetype = "dashed") +
  annotate("text", x = plotEst$lambda[1]+1.4, y = plotEst$X22[1], label = "MCSIMEX estimate") +
  annotate("text", x = plotEst$lambda[2]+1.4, y = plotEst$X22[2], label = "Naïve estimate") +
  theme_bw() +
  ylab(expression(beta)) +
  theme(axis.text.y = element_text(angle = 90)) +
  xlab(expression(lambda + 1))
simex.plot
```

```{r, echo = FALSE}
ggsave(simex.plot, file = "Output/Figures/simex.plot.eps",
       height = 5, width = 7, dpi = 300)
```


A range of scenarios similar to the one described above was tested by varying both the number of training data pairs $n = (200, 500, 1000)$ and the class probability $\pi_0 = (0.2, 0.5)$ . Each scenario was repeated 1000 times to calculate the mis-classification matrix by monte carlo simulation. We performed unsupervised clustering with GMM and Kmeans.

```{r simulation}
rerun <- FALSE

# Number of simulated experiments
nSim    <- 1000
nSample <- c(200, 500, 1000)
probs   <- c(0.2, 0.5)
method  <- c("GMM", "Kmeans")

# list for results
simResults <- list()
parPackages <- c("simex", "mixtools", "mclust", "arrangements")

if(!file.exists("GeneratedData/sim_results_binom.RData") || rerun){
  # Set up parallel computations
  registerDoParallel(cores = 30)
  registerDoRNG(seed = 123)

  for(meth in method){
    for(classProb in probs){
      for(sampleSize in nSample){
        simResults[[paste(meth)]][[paste(classProb)]][[paste(sampleSize)]] <-
          foreach(i=1:nSim,
                  .combine = "rbind",
                  .errorhandling = "pass",
                  .packages = parPackages) %dopar% {
                      simBinom(n   = sampleSize,
                               pi0 = classProb,
                               alpha = alpha,
                               beta = beta,
                               mu0 = mu0,
                               mu1 = mu1,
                               method = meth)
        }
      }
    }
  }
  save(simResults, file = "GeneratedData/sim_results_binom.RData")
  stopImplicitCluster()
}
```

Summarize the result of the 1000 simulations. We see clearly an attenuation under mislabeling, and considerable improvement under the MC-SIMEX approach.
```{r simulations}
load("GeneratedData/sim_results_binom.RData")

## Summarize results
summarizeResultsList <- function(x) summarizeResults(x, alpha = alpha, beta = beta, digits = 2)
rs <- lapply(simResults, function(x) lapply(x, function(x) lapply(x, summarizeResultsList)))

balanced <- cbind(rep(names(rs$GMM$`0.5`), each = 6),
                      do.call(rbind, rs$GMM$`0.5`),
                      do.call(rbind, rs$Kmeans$`0.5`))
unbalanced <- cbind(rep(names(rs$GMM$`0.2`), each = 6),
                      do.call(rbind, rs$GMM$`0.2`),
                      do.call(rbind, rs$Kmeans$`0.2`))

kable(balanced, 
      col.names = c("nSample", "GMM True", "GMM Naive", "GMM Simex",
                    "KM True", "KM Naive", "KM Simex"),
      caption = "Binomial simulation results with balanced classes")
kable(unbalanced, 
      col.names = c("nSample", "GMM True", "GMM Naive", "GMM Simex",
                    "KM True", "KM Naive", "KM Simex"),
      caption = "Binomial simulation results with unbalanced classes")

```


```{r, echo = FALSE, warning = FALSE}
## Create Latex tables
tBal <- Hmisc::latex(balanced[,-1],
             title = "",
             cgroup = c("GMM", "Kmeans"),
             n.cgroup = c(3,3),
             rgroup = c(200,500,1000),
             n.rgroup = c(6,6,6),
             file = "Output/Tables/balanced.tex",
             colheads = c(rep(c("True", "Naive","Simex"),2)),
             caption = "Results from balanced simulations",
             label = "simResults:balanced"
             )
tUbal <- Hmisc::latex(unbalanced[,-1],
             title = "",
             cgroup = c("GMM", "Kmeans"),
             n.cgroup = c(3,3),
             rgroup = c(200,500,1000),
             n.rgroup = c(6,6,6),
             file = "Output/Tables/unbalanced.tex",
             colheads = c(rep(c("True", "Naive","Simex"),2)),
             caption = "Results from unbalanced simulations",
             label = "simResults:unbalanced"
             )
```


```{r, echo = FALSE, warning = FALSE}
## Create Latex tables
tBal <- Hmisc::latex(balanced[-c(3,4,9,10,15,16),-c(1,5)],
             title = "",
             cgroup = c("","GMM", "Kmeans"),
             n.cgroup = c(1,2,2),
             rgroup = c(200,500,1000),
             n.rgroup = c(4,4,4),
             file = "Output/Tables/balancedNew.tex",
             colheads = c(rep(c("True", "Naive","Simex"),2)),
             caption = "Results from balanced simulations",
             label = "simResults:balanced"
             )
tUbal <- Hmisc::latex(unbalanced[-c(3,4,9,10,15,16),-c(1,5)],
             title = "",
             cgroup = c("","GMM", "Kmeans"),
             n.cgroup = c(1,2,2),
             rgroup = c(200,500,1000),
             n.rgroup = c(4,4,4),
             file = "Output/Tables/unbalancedNew.tex",
             colheads = c(rep(c("True", "Naive","Simex"),2)),
             caption = "Results from unbalanced simulations",
             label = "simResults:unbalanced"
             )

```


# Simulation with survival data
Now we want to see if how MCSIMEX works with survival data. We generate $n = 200$, $n = 500$ or $n = 1000$ training datapoints from two classes using a binomal distribution with parameter $\pi = 0.5$ and add noise to get misclassification probabilities of $0.1$, $0.2$ or $0.3$ for both classes. The outcome is assumed to be exponentially distributed with rate $\beta*(1+\mbox{class})$ and the censoring time exponentially distributed with reate 0.5. We estimate the hazard ratio of the two classes using the true classlabels, the misspecified classlabels, and the simex corrected hazard ratio using a misclassificationsmatrix with the true misclassification rates. 
```{r}
rerun <- FALSE

# Simulated experiments
nSim <- 1000
nSample <- c(200,500,1000)
mpList  <- c(0.1, 0.2, 0.3)

# Function to parallelize
simFuncCox <- function(n = 1000, mp = 0.3){
  d = my.sim(n = n, mp = mp)
  
  hatPi = matrix(c(1-mp, mp, mp, 1-mp), nrow = 2, byrow = T) # We use here the truth
  colnames(hatPi) <- levels(factor(d$X1))
  rownames(hatPi) <- levels(factor(d$X1))
  
  d$surv <- Surv(d$time,d$status)
  model.true  <- coxph(surv ~ X1,
                 data  = d)
  
  model.naive <- coxph(surv ~ X2,
                 data  = d,
                 model = TRUE)
  
  model.simex <- mcsimex(model = model.naive,
                         SIMEXvariable = "X2",
                         mc.matrix = hatPi,
                         asymptotic = FALSE)
  results <- c(coefficients(model.true),
           coefficients(model.naive),
           model.simex$coefficients,
           sqrt(vcov(model.true)),
           sqrt(vcov(model.naive)),
           sqrt(model.simex$variance.jackknife))
  names(results) <- c("True", "Naive", "Simex",
                      "True.sd", "Naive.sd", "Simex.sd")
  
  return(results)
}

if(!file.exists("GeneratedData/sim_results_cox.RData") || rerun){
  simResultsCox <- list()
  registerDoParallel(cores = 30)
  registerDoRNG(seed = 123)
  
  for(mp in mpList){
    for(sampleSize in nSample){
      simResultsCox[[paste(mp)]][[paste(sampleSize)]] <-
        foreach(i=1:nSim,
                .combine = "rbind",
                .errorhandling = "pass",
                .packages = parPackages) %dopar% {
                  simFuncCox(n = sampleSize, mp = mp)
                }
    }
  }
  stopImplicitCluster()
  save(simResultsCox, file = "GeneratedData/sim_results_cox.RData")
} else{
  load("GeneratedData/sim_results_cox.RData")
}
```

Summarize results
```{r}
coxTable <- lapply(simResultsCox, function(x) lapply(x, summarizeResultsCox))
coxTable <- cbind(rep(nSample, each = 3),
            do.call(rbind, coxTable[["0.1"]]),
            do.call(rbind, coxTable[["0.2"]]),
            do.call(rbind, coxTable[["0.3"]])
)
kable(coxTable, caption = "Simulation results with Cox for misclassification probability of 0.1, 0.2 or 0.3") %>% add_header_above(c(" " = 2, "0.1" = 3, "0.2" = 3, "0.3" = 3))
```

```{r, echo = FALSE, warning = FALSE}
## Create Latex tables
tCox <- Hmisc::latex(coxTable[,-1],
             title = "",
             cgroup = c("mp = 0.1", "mp = 0.2","mp = 0.3"),
             n.cgroup = c(3,3,3),
             rgroup = c(200,500,1000),
             n.rgroup = c(3,3,3),
             file = "Output/Tables/cox.tex",
             colheads = c(rep(c("True", "Naive","Simex"),3)),
             caption = "Results from cox estimation of 1000 simulations using a misclassification probability (mp) of 0.1, 0.2 or 0.3",
             label = "simResults:cox"
           )
```

```{r, echo = FALSE, warning = FALSE}
## Create Latex tables
tCox <- Hmisc::latex(coxTable[-c(2,5,8),-c(1,5,8)],
             title = "",
             cgroup = c("","mp = 0.1", "mp = 0.2","mp = 0.3"),
             n.cgroup = c(1,2,2,2),
             rgroup = c(200,500,1000),
             n.rgroup = c(2,2,2),
             file = "Output/Tables/cox_new.tex",
             colheads = c("True", rep(c("Naive","Simex"),3)),
             caption = "Results from cox estimation of 1000 simulations using a misclassification probability (mp) of 0.1, 0.2 or 0.3",
             label = "simResults:cox"
           )

```


# Cancer subclassification
Load and data management of multiple myeloma data, GES4581. The raw CEL files are available from a later study and stored in GEO repositoty GSE24080, so we download these and use metadata from the two studys to match them by patientID.
```{r}
if(!file.exists("ExternalData/MAQCII_GSE24080/download.done")){
  getGEOSuppFiles("GSE24080",baseDir = "ExternalData/MAQCII_GSE24080",makeDirectory = FALSE)
  system("tar -xvf ExternalData/MAQCII_GSE24080/GSE24080_RAW.tar -C ExternalData/MAQCII_GSE24080/")
  system("rm ExternalData/MAQCII_GSE24080/GSE24080_RAW.tar")
  system("touch ExternalData/MAQCII_GSE24080/download.done")
}
```

Load the preprocessed GSE datasets for the new and old study to cross reference IDs.
```{r}
# Load the two datasets.
if(!file.exists("ExternalData/UAMS_GSE4581/gse4581.RData")){
  gse4581 <- getGEO("GSE4581", GSEMatrix = TRUE,destdir = "ExternalData/UAMS_GSE4581")[[1]]
  save(gse4581, file = "ExternalData/UAMS_GSE4581/gse4581.RData")
}
load("ExternalData/UAMS_GSE4581/gse4581.RData")

if(!file.exists("ExternalData/MAQCII_GSE24080/gse24080.RData")){
  gse24080 <- getGEO("GSE24080", GSEMatrix = TRUE,destdir = "ExternalData/MAQCII_GSE24080")[[1]]
  save(gse24080,file = "ExternalData/MAQCII_GSE24080/gse24080.RData")
}
load("ExternalData/MAQCII_GSE24080/gse24080.RData")

# Cross reference the IDs.
meta_4581  <- pData(gse4581) %>%
  mutate(
    PatientID = gsub("U133Plus-","",title) %>% 
                gsub(" \\(.*","",.) %>% 
                gsub("-.*","",.)) 

meta_24080 <- pData(gse24080) %>%
  mutate(
    PatientID = gsub("-.*","",title) %>%
    sapply(.,function(i){
      Str <- strsplit(i,split = "")[[1]]
      
      if (Str[2] == "0"){
        out <- paste(Str[-2],collapse = "")
      } else {
        out <- paste(Str,collapse = "")
      }
      return(out)
    }) %>% as.character())

# Merge the two IDs.
df_merg    <- left_join(select(meta_4581,PatientID,geo_accession),
                        select(meta_24080,PatientID,geo_accession),
                        by = "PatientID",suffix = c("_4581","_24080"))

# Find the filenames matching. Note that only 408 of 414 had a match.
all_files  <- list.files("ExternalData/MAQCII_GSE24080",pattern = "CEL.gz",full.names = T)
files_zhan <- df_merg$geo_accession_24080 %>%
  sapply(.,function(i){
    tmp <- grep(pattern = i,x = all_files,value = T)
    ifelse(all(is.na(tmp)),NA,tmp)
  }) %>%
  {.[!is.na(.)]}

# Test that it is the same data!
wh <- names(files_zhan) %>% sapply(.,function(i){which(i == df_merg$geo_accession_24080)})

x1 <- log2(exprs(gse4581))[,df_merg$geo_accession_4581[wh]]
x2 <- exprs(gse24080)[,df_merg$geo_accession_24080[wh]]

cors <- mapply(function(a,b){
  cor(x1[,a],x2[,b],use = "complete.obs")
},a = colnames(x1),b = colnames(x2)) %>% `names<-`(colnames(x2))
  
boxplot(cors) # All but one sample have r > 0.9999

# Remove the file with bad correlation.
bad        <- names(which.min(cors))
files_zhan <- files_zhan[!grepl(bad,files_zhan)]
```

MAS5 normalize the raw data from zhan et al. 2006.
```{r}
if(!file.exists("GeneratedData/Zhan_mas5.rds")){

  # Read the CEL files into R.
  zhan_data <- read.affybatch(files_zhan,compress = T)

  # MAS5 normalization.
  zhan_mas5 <- mas5(zhan_data)
  
  # Present/absent calls.
  zhan_PA   <- mas5calls(zhan_data)
  
  # Save relevant data.
  saveRDS(zhan_mas5,"GeneratedData/Zhan_mas5.rds")
  saveRDS(zhan_PA,"GeneratedData/Zhan_PA.rds")
}
zhan_mas5 <- readRDS("GeneratedData/Zhan_mas5.rds")
zhan_PA   <- readRDS("GeneratedData/Zhan_PA.rds")
```

Curate the metadata from Zhan et al. 2006.
```{r}
pheno.mm <- pData(gse4581)

## Get treatment regime

pheno.mm$regime <- "TT2"
pheno.mm$regime[grep("TT3", pheno.mm$title)] <- "TT3"

## Get the eventtime, event, and class phenotype and removing annoying extra chars

pheno.mm$eventtime <- as.character(pheno.mm$characteristics_ch1.2)
pheno.mm$eventtime <- sub("\\[SURTIM=", "", pheno.mm$eventtime)
pheno.mm$eventtime <- as.numeric(lapply(strsplit(pheno.mm$eventtime, " "),
                                        function(x){x[1]}))

pheno.mm$event <- as.character(pheno.mm$characteristics_ch1)
pheno.mm$event <- sub("\\[SURIND=", "", pheno.mm$event)
pheno.mm$event <- as.numeric(lapply(strsplit(pheno.mm$event, " "),
                                        function(x){x[1]}))

pheno.mm$class <-  pData(gse4581)$characteristics_ch1.8
pheno.mm$class <- sub("\\[Subgrp7=", "", pheno.mm$class)
pheno.mm$class <- factor(sub("\\]", "", pheno.mm$class))
pheno.mm$class.no <- as.numeric(pheno.mm$class) 

pheno.mm$PatientID <- gsub("U133Plus-","",pheno.mm$title) %>% 
                      gsub(" \\(.*","",.) %>% 
                      gsub("-.*","",.)
pheno.mm <- select(pheno.mm,PatientID,regime,eventtime,event,class,class.no)
```

Merge the phenodata with expression data, such that IDs match.
```{r}
CELtoPatientID <- pData(zhan_mas5) %>% rownames_to_column("CEL") %>%
  mutate(
    sample = gsub("-.*","",CEL) %>%
    sapply(.,function(i){strsplit(i,split = "_")[[1]][2]}) %>%
    sapply(.,function(i){
      Str <- strsplit(i,split = "")[[1]]
      
      if (Str[2] == "0"){
        out <- paste(Str[-2],collapse = "")
      } else {
        out <- paste(Str,collapse = "")
      }
      return(out)
    }) %>% as.character())

zhan_pheno <- left_join(CELtoPatientID,pheno.mm,by = c("sample" = "PatientID")) %>% column_to_rownames("CEL")

pData(zhan_mas5) <- zhan_pheno
fData(zhan_mas5) <- fData(gse4581)
```

Now perform the filtering according to Zhan et al. 2006.
```{r}
# 1. Keep probesets called present in >3% of samples.---------------------------
keeps <- exprs(zhan_PA) %>% 
  apply(.,1,`==`,"P") %>% 
  apply(.,2,function(x){sum(x) > floor(length(x)*0.03)})
  
zhan_proc <- zhan_mas5[keeps,]

# 2. log2 transform.------------------------------------------------------------
exprs(zhan_proc) <- log2(exprs(zhan_proc))

# 3. Retain only affymetrix probesets with a standard deviation > 1.34 in the training dataset.
exprs.train <- exprs(zhan_proc[,zhan_proc$regime == "TT2"])
keeps       <- apply(exprs.train,1,sd) > 1.34

zhan_proc   <- zhan_proc[keeps,]

# 4. For multiple affymetrix probesets matching the same gene, retain the one with highest variance.

# Get the set of all gene symbols.
geneIDs <- fData(zhan_proc)$`Gene Symbol` %>%
  lapply(.,function(x){strsplit(x,split = " /// ")[[1]]}) %>%
  unlist() %>%
  unique()
geneIDs <- unique(fData(zhan_proc)$`Gene Symbol`)

# Go through each gene iteratively.
tmp   <- fData(zhan_proc)$`Gene Symbol`
keeps <- sapply(geneIDs,function(gene){
  wh    <- which(tmp == gene)
  x_sub <- exprs(zhan_proc)[wh,,drop = F]
  names(which.max(apply(x_sub,1,sd)))
})

zhan_proc <- zhan_proc[keeps,]
```

Save the filtered data.
```{r}
saveRDS(zhan_proc,"GeneratedData/Zhan_filtered.rds")
```

Check that the data is similar to GSE4581 
```{r}
wh_probes <- intersect(rownames(zhan_proc),rownames(gse4581))

df_merg <- pData(zhan_proc) %>% rownames_to_column("CEL") %>%
  left_join(.,meta_4581,by = c("sample" = "PatientID")) %>%
  select(CEL,geo_accession,sample,title)

cors <- sapply(1:nrow(df_merg),function(i){
  cor(exprs(zhan_proc)[wh_probes,df_merg$CEL[i]],log2(exprs(gse4581)[wh_probes,df_merg$geo_accession[i]]),use = "complete.obs")
})
boxplot(cors)
```

## Hierarchical clustering 
We try to cluster the data and cut at 7 groups to see if we get the same results as Zhan 2006.
```{r}
#d <- dist(t(exprs(zhan_proc[,zhan_proc$regime == "TT2"])))
#d <- as.dist(1- cor(exprs(zhan_proc[,zhan_proc$regime == "TT2"])))
d <- 1-proxy::dist(exprs(zhan_proc[,zhan_proc$regime == "TT2"]), method = ucor, by_rows = F)

clust <- hclust(d, method = "ward.D")

labs <- cutree(clust,k = 7) %>% data.frame(clust = .)

tmp <- merge(pData(zhan_proc),labs,by = 0)

conf.hclst <- table(tmp$class,tmp$clust)

# Reorder to get "diagonal" matrix
conf.hclst <- conf.hclst[opt(conf.hclst), ]
conf.hclst
sum(diag(conf.hclst))/sum(conf.hclst)
plot(clust, label = F)
```

```{r, echo = F, eval = F}
### This chunk is an attempt to recreate classes using the same
### clustering algorhithm as in Zhan (2006). We could only find
### an implementation in the python bio.tools package, so this is
### done using the reticulate R library. Resulst were poor, so we
### did not use them for the paper.

### Python
library(reticulate)
pyData <- exprs(zhan_proc[,zhan_proc$regime == "TT2"])
use_python("/usr/bin/python3", required = T)

import("Bio")
py_run_string("from Bio.Cluster import treecluster")
## Use the bioPython library to do pairwise centroid-linkage clustering (method = c) 
## One the columns of the GEP matrix (transpose = 1), with 
## The centered correlation measure for distance (dist = c)
py_run_string("tree = treecluster(data = r.pyData, method = 'c', dist = 'c', transpose = 1)")
py_run_string("clusterid = tree.cut(nclusters = 7)")

## Compare classes
labs.py <- py$clusterid
names(labs.py) <- colnames(pyData)
labs.py <- data.frame(labs.py)

tmp.py <- merge(pData(zhan_proc), labs.py, by = 0)

conf.hclst.py <- table(tmp.py$class, tmp.py$labs.py)

# Reorder to get "diagonal" matrix
conf.hclst.py <- conf.hclst.py[opt(conf.hclst.py), ]
conf.hclst.py
sum(diag(conf.hclst.py))/sum(conf.hclst.py)
```

## GMM clustering
We try to cluster the data and cut at 7 groups to see if we get the same results as Zhan 2006.
```{r}
fit.GMM <- Mclust(t(exprs(zhan_proc[,zhan_proc$regime == "TT2"])),G = 7)
labs <- fit.GMM$classification %>% data.frame(clust = .)
pData.gmm <- merge(pData(zhan_proc),labs,by = 0)

conf.GMM <- table(pData.gmm$class,pData.gmm$clust)

# Reorder to get "diagonal" matrix
conf.GMM <- conf.GMM[opt(conf.GMM), ]
conf.GMM.acc <- sum(diag(conf.GMM))/sum(conf.GMM)
conf.GMM
conf.GMM.acc
```

Use most likely groups from confusion matrix to label GMM groups
```{r}
pData.gmm$gmm.class <- row.names(conf.GMM)[pData.gmm$clust]
conf.gmm.train <- table(pData.gmm$class, pData.gmm$gmm.class)
```

PCA plot for GMM classes
```{r}
# Plot of prefiltered data
pca.mm <- princomp(exprs(zhan_proc[,zhan_proc$regime == "TT2"]))

# Superimposing classes in a pca plot
pca.plotdata <- data.frame("PC1" = pca.mm[[2]][,1],
                           "PC2" = pca.mm[[2]][,2],
                           "Class" = factor(pData.gmm$gmm.class))
fig.mm.pca <- ggplot(pca.plotdata, aes(x = PC1, y = PC2, col = Class)) +
  geom_point() + stat_ellipse() +
  ggtitle("PCA plot for GSE4581")
fig.mm.pca
ggsave(fig.mm.pca, file = "Output/Figures/fig_mm_pca.pdf",
       height = 7, width = 7, dpi = 300)
```


Plot survival curves for fitted groups
```{r}

pData.gmm$OS <- Surv(pData.gmm$eventtime,pData.gmm$event)
fit1 <- survfit(pData.gmm$OS~pData.gmm$class)
fit2 <- survfit(pData.gmm$OS~pData.gmm$gmm.class)
names(fit1$strata) <- gsub("class=", "", names(fit1$strata))
names(fit2$strata) <- gsub("gmm.class=", "", names(fit2$strata))
zhan7_train_plot <- ggsurvplot(fit1, data = pData.gmm, title = "Zhan Classes", pval = T)
gmm7_train_plot <- ggsurvplot(fit2, data = pData.gmm, title = "GMM Classes", pval = T)
arrange_ggsurvplots(list(zhan7_train_plot, gmm7_train_plot))
```

Try in test data
```{r}
test.labs <- predict(fit.GMM, t(exprs(zhan_proc[,zhan_proc$regime == "TT3"])))
test.labs2 <- data.frame("clust" = test.labs$classification)
row.names(test.labs2) <- row.names(test.labs$z)
pData.gmm.test <- merge(pData(zhan_proc),test.labs2,by = 0)
pData.gmm.test$gmm.class <- row.names(conf.GMM)[pData.gmm.test$clust]
conf.GMM.val <- table(pData.gmm.test$class, pData.gmm.test$gmm.class)
conf.GMM.val.acc <- sum(diag(conf.GMM.val)) / sum(conf.GMM.val)
conf.GMM.val
```

KM plots in test data
```{r}
pData.gmm.test$OS <- Surv(pData.gmm.test$eventtime,pData.gmm.test$event)
fit1 <- survfit(pData.gmm.test$OS~pData.gmm.test$class)
fit2 <- survfit(pData.gmm.test$OS~pData.gmm.test$gmm.class)
names(fit1$strata) <- gsub("class=", "", names(fit1$strata))
names(fit2$strata) <- gsub("gmm.class=", "", names(fit2$strata))
zhan7_val_plot <- ggsurvplot(fit1, data = pData.gmm.test, title = "Zhan Classes (Test data)", pval = T)
gmm7_val_plot <- ggsurvplot(fit2, data = pData.gmm.test, title = "GMM Classes (Test data)", pval = T)
arrange_ggsurvplots(list(zhan7_val_plot, gmm7_val_plot))
```


## Cox regression in cancer data
Define low and high risk groups and do cox regression in training data
```{r}
pData.gmm$zhan.risk <- factor(ifelse(pData.gmm$class %in% c("PR", "MF", "MS"), "High Risk", "Low Risk"),
                            levels = c("Low Risk", "High Risk"))
pData.gmm$gmm.risk <- factor(ifelse(pData.gmm$gmm.class %in% c("PR", "MF", "MS"), "High Risk", "Low Risk"),
                           levels = c("Low Risk", "High Risk"))

coxph(OS~zhan.risk, data = pData.gmm)
coxph(OS~gmm.risk, data = pData.gmm)
```

Kaplan-Meier plot for low vs high risk
```{r}
fit1 <- survfit(pData.gmm$OS~pData.gmm$zhan.risk)
fit2 <- survfit(pData.gmm$OS~pData.gmm$gmm.risk)
names(fit1$strata) <- gsub("zhan.risk=", "", names(fit1$strata))
names(fit2$strata) <- gsub("gmm.risk=", "", names(fit2$strata))
zhanRisk_train_plot <- ggsurvplot(fit1, data = pData.gmm, title = "Zhan Risk Class", pval = T)
gmmRisk_train_plot <- ggsurvplot(fit2, data = pData.gmm, title = "GMM Risk Class", pval = T)
arrange_ggsurvplots(list(zhanRisk_train_plot, gmmRisk_train_plot))
```


Try in test data
```{r}
pData.gmm.test$zhan.risk <- factor(ifelse(pData.gmm.test$class %in% c("PR", "MF", "MS"), "High Risk", "Low Risk"),
                            levels = c("Low Risk", "High Risk"))
pData.gmm.test$gmm.risk <- factor(ifelse(pData.gmm.test$gmm.class %in% c("PR", "MF", "MS"), "High Risk", "Low Risk"),
                           levels = c("Low Risk", "High Risk"))

coxph(OS~zhan.risk, data = pData.gmm.test)
coxph(OS~gmm.risk, data = pData.gmm.test)
```

Kaplan-Meier plot for low vs high risk
```{r}
fit1 <- survfit(pData.gmm.test$OS~pData.gmm.test$zhan.risk)
fit2 <- survfit(pData.gmm.test$OS~pData.gmm.test$gmm.risk)
names(fit1$strata) <- gsub("zhan.risk=", "", names(fit1$strata))
names(fit2$strata) <- gsub("gmm.risk=", "", names(fit2$strata))
zhanRisk_val_plot <- ggsurvplot(fit1, data = pData.gmm.test,
                                title = "Zhan Risk Class (test data)", pval = T)
gmmRisk_val_plot <- ggsurvplot(fit2, data = pData.gmm.test,
                               title = "GMM Risk Class (test data)", pval = T)
arrange_ggsurvplots(list(zhanRisk_val_plot, gmmRisk_val_plot))
```

Dump plots for publication
```{r}
trainSurvPlots <- arrange_ggsurvplots(list(zhan7_train_plot + labs(tag = "A"),
                                           zhanRisk_train_plot  + labs(tag = "C"),
                                           gmm7_train_plot + labs(tag = "B"),
                                           gmmRisk_train_plot + labs(tag = "D")),
                                      ncol = 2, nrow = 2,
                                      print = F)
valSurvPlots <- arrange_ggsurvplots(list(zhan7_val_plot + labs(tag = "A"),
                                           zhanRisk_val_plot + labs(tag = "B"),
                                           gmm7_val_plot + labs(tag = "C"),
                                           gmmRisk_val_plot + labs(tag = "D")),
                                      ncol = 2, nrow = 2,
                                    print=F)

ggsave(trainSurvPlots, file = "Output/Figures/trainSurvPlots.pdf",
       height = 7, width = 7, dpi = 300)
ggsave(trainSurvPlots, file = "Output/Figures/trainSurvPlots.eps",
       height = 7, width = 7, dpi = 350)
ggsave(valSurvPlots, file = "Output/Figures/valSurvPlots.pdf",
       height = 7, width = 7, dpi = 300)
```


```{r, echo = FALSE}
## Dump tables
tab.conf.mm <- Hmisc::latex(conf.GMM,
             title = "",
             file = "Output/Tables/conf.mm.tex",
             caption = paste("Confusion matrix for training set of GSE4581, accuracy = ",
                             round(conf.GMM.acc,2)),
             label = "confmm:train")

tab.conf.mm <- Hmisc::latex(conf.GMM.val,
             title = "",
             file = "Output/Tables/conf.mm.val.tex",
             caption = paste("Confusion matrix for validation set of GSE4581, accuracy = ",
                             round(conf.GMM.val.acc,2)),
             label = "confmm:val")
```

Bootstrap misclassification matrix
```{r}
# Misclaffification matrix
ct <- pData.gmm$gmm.class

# 0.632 bootstrap
n = nrow(t(exprs(zhan_proc[,zhan_proc$regime == "TT2"])))
n.boot = round((1-1/exp(1))*n)

bootsFile <- "GeneratedData/mc.boots.GMM.RData"
if(!file.exists(bootsFile)){
  # Set up parallel computations
  registerDoParallel(cores = 30)
  registerDoRNG(seed = 321)
  
  r.boot <- foreach(i=1:1000, .combine = "+")  %dopar% {
    bootConf(n.boot = n.boot,
             data = t(exprs(zhan_proc[,zhan_proc$regime == "TT2"])),
             orig.class = ct,
             method = "GMM")
  }
  save(r.boot, file = bootsFile)
  stopImplicitCluster()
} else load(bootsFile)

## Build hatPi matrix from bootstraps
hatPi <- sweep(r.boot, MARGIN = 2, FUN="/", STATS=colSums(r.boot))
rownames(hatPi) <- colnames(hatPi) #Only rename rows, columns are original classes and seem okay
xhatPi <- build.mc.matrix(as.matrix(hatPi))
kable(round(hatPi*100),
      caption = "Bootstrapped misclassification with GMM",
      row.names = T)
```

Fit cox model of low vs highrisk. First we build a misclassification matrix with only two classes
```{r}
## Construct 2x2 misclassification matrix
high <- which(colnames(xhatPi) %in% c("PR", "MF", "MS"))

smallXhatPi <- matrix(data = c(sum(r.boot[-high,-high]),
                               sum(r.boot[-high,high]),
                               sum(r.boot[high,-high]),
                               sum(r.boot[high,high])),
                               ncol = 2, nrow = 2, byrow = T)

smallXhatPi <- sweep(smallXhatPi, MARGIN = 2, FUN="/", STATS=colSums(smallXhatPi))
row.names(smallXhatPi) <- colnames(smallXhatPi) <- c("Low Risk", "High Risk")
kable(smallXhatPi)
```

Then we fit the cox model and do SIMEX correction.
```{r}
cox.gmm  <- coxph(OS~gmm.risk, model = TRUE, data = pData.gmm)
cox.gmm.simex <- mcsimex(cox.gmm,
                         SIMEXvariable = "gmm.risk",
                         mc.matrix = smallXhatPi,
                         asymptotic = FALSE)
```

Try Simex with bootstraps instead
```{r}
rerun <- FALSE
if(!file.exists("GeneratedData/boot.Simex.GMM.RData") || rerun){
  # 0.632 bootstrap
  n = nrow(t(exprs(zhan_proc[,zhan_proc$regime == "TT2"])))
  n.boot = round((1-1/exp(1))*n)

  # Set up parallel computations
  registerDoParallel(cores = 30)
  registerDoRNG(seed = 123)
  
  # Do simex bootstraps in parallel
  simex.boot <- foreach(i=1:1000, .combine = "c")  %dopar% {
    simexBoot(n.boot = FALSE, #Set a number of samples without replacement, or norm bootstrap
              orig.class = pData.gmm$gmm.class,
              edata = t(exprs(zhan_proc[,zhan_proc$regime == "TT2"])),
              pdata = pData.gmm,
              SIMEXvar = "gmm.risk")
    }
  save(simex.boot, file = "GeneratedData/boot.Simex.GMM.RData")
  stopImplicitCluster()
} else load("GeneratedData/boot.Simex.GMM.RData")
```

Train model with Zhan risk classes for comparisson
```{r}
cox.zhan.sum <- summary(coxph(OS ~ zhan.risk, data = pData.gmm))
```


Build table with results
```{r}
## Build results matrix
cox.gmm.sum <- summary(cox.gmm)
cox.gmm.simex.sum <- summary(cox.gmm.simex)

results <- matrix(0, nrow = 4, ncol =4)
# Zhan Naive
results[1,1] <- cox.zhan.sum$coefficients[2]
results[1,2] <- cox.zhan.sum$conf.int[3]
results[1,3] <- cox.zhan.sum$conf.int[4]
results[1,4] <- cox.zhan.sum$coefficients[5]


# Naive
results[2,1] <- cox.gmm.sum$coefficients[2]
results[2,2] <- cox.gmm.sum$conf.int[3]
results[2,3] <- cox.gmm.sum$conf.int[4]
results[2,4] <- cox.gmm.sum$coefficients[5]

# Simex
results[3,1] <- exp(cox.gmm.simex$coefficients)[1]
results[3,2] <- exp(cox.gmm.simex$coefficients -1.96 * sqrt(cox.gmm.simex$variance.jackknife))
results[3,3] <- exp(cox.gmm.simex$coefficients +1.96 * sqrt(cox.gmm.simex$variance.jackknife))
results[3,4] <- cox.gmm.simex.sum$coefficients$jackknife[4]

# Simex bootstrap
df <- cox.gmm$n - length(coef(cox.gmm))
results[4,1] <- exp(mean(simex.boot))
results[4,2] <- exp(mean(simex.boot) - 1.96 * sd(simex.boot))
results[4,3] <- exp(mean(simex.boot) + 1.96 * sd(simex.boot))
results[4,4] <- 2 * pt(abs(mean(simex.boot)/sd(simex.boot)), df, lower.tail = F) 

row.names(results) <- c("Zhan classes - Naïve","GMM - Naïve", "GMM - Simex (Average MC)", "GMM - Simex (Full bootstrap)")
colnames(results)  <- c("HR", "Lower 95", "Upper 95", "P-value")
kable(results, digits = 2,
      caption = "Hazard ratio of high vs low risk groups from Zhan2006 with the naïve and simex corrected models using the GMM classes.")
```

Latex table with results
```{r}
resultsLatex <- results
resultsLatex[,1:3] <- round(resultsLatex[,1:3], 2)
tab.cox.mm <- Hmisc::latex(resultsLatex,
             title = "",
             file = "Output/Tables/cox.mm.tex",
             caption = "Hazard ratio of high vs low risk groups in the training set from Zhan2006 with the naïve and simex corrected models using the GMM classes.",
             label = "tab.cox.mm")

```


### Try with 7 classes
```{r, echo = T, eval = T}
# chunk disabled
pData.gmm$gmm.class <- as.factor(pData.gmm$gmm.class)
model <- coxph(OS ~ gmm.class, data = pData.gmm, model = T)
model.simex <- mcsimex(model,
                       SIMEXvariable = "gmm.class",
                       mc.matrix = xhatPi,
                       asymptotic = F)
full.cox.naive <- summary(model)
full.cox.simex <- summary(model.simex)
```

Tabulate number of events per class to investigate poor performance of cox ph.
```{r}
table(pData.gmm$gmm.class, pData.gmm$event)
```

Build Table
```{r}
results <- matrix(0, nrow = 6, ncol = 6)
results[,1] <- full.cox.naive$coefficients[,2]
results[,2] <- full.cox.naive$conf.int[,3]
results[,3] <- full.cox.naive$conf.int[,4]

results[,4] <- exp(full.cox.simex$coefficients$jackknife[,1])
results[,5] <- exp(full.cox.simex$coefficients$jackknife[,1] 
                   -1.96 * full.cox.simex$coefficients$jackknife[,2])
results[,6] <- exp(full.cox.simex$coefficients$jackknife[,1] 
                   +1.96 * full.cox.simex$coefficients$jackknife[,2])
row.names(results) <- c(row.names(full.cox.naive$coefficients))
colnames(results)  <- rep(c("HR", "Lower 95", "Upper 95"),2)

kable(results, digits = 2,
      caption = "Hazard ratio of GMM classes using the naive or simex corrected model") %>%
  add_header_above(c(" "=1,"Naive" = 3, "Simex" = 3))
```

Print sessioninfo
```{r}
options(width = 100)
devtools::session_info()
```
